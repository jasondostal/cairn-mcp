# Cairn: Persistent Memory for Agents and Humans
# Copy to .env and fill in values

# PostgreSQL
CAIRN_DB_HOST=cairn-db
CAIRN_DB_PORT=5432
CAIRN_DB_NAME=cairn
CAIRN_DB_USER=cairn
CAIRN_DB_PASS=cairn-dev-password

# LLM Backend: "ollama" (default), "bedrock", "gemini", or "openai"
# The "openai" backend works with any OpenAI-compatible API: OpenAI, Groq,
# Together, Mistral, LM Studio, vLLM, etc. Just set the base URL.
CAIRN_LLM_BACKEND=ollama

# Ollama (default â€” free, local, no API key)
CAIRN_OLLAMA_URL=http://host.docker.internal:11434
CAIRN_OLLAMA_MODEL=qwen2.5-coder:7b

# AWS Bedrock
# CAIRN_BEDROCK_MODEL=us.meta.llama3-2-90b-instruct-v1:0
# AWS_ACCESS_KEY_ID=
# AWS_SECRET_ACCESS_KEY=
# AWS_DEFAULT_REGION=us-east-1

# Google Gemini (free tier available)
# CAIRN_GEMINI_MODEL=gemini-2.0-flash
# CAIRN_GEMINI_API_KEY=

# OpenAI-compatible (works with OpenAI, Groq, Together, Mistral, LM Studio, vLLM)
# CAIRN_OPENAI_BASE_URL=https://api.openai.com
# CAIRN_OPENAI_MODEL=gpt-4o-mini
# CAIRN_OPENAI_API_KEY=

# Transport: "stdio" (docker exec) or "http" (network)
CAIRN_TRANSPORT=http
CAIRN_HTTP_HOST=0.0.0.0
CAIRN_HTTP_PORT=8000

# Enrichment: set to "false" to disable LLM enrichment on store
CAIRN_ENRICHMENT_ENABLED=true

# LLM Capabilities: each can be toggled independently
CAIRN_LLM_QUERY_EXPANSION=true
CAIRN_LLM_RELATIONSHIP_EXTRACT=true
CAIRN_LLM_RULE_CONFLICT_CHECK=true
CAIRN_LLM_SESSION_SYNTHESIS=true
CAIRN_LLM_CONSOLIDATION=true
CAIRN_LLM_CONFIDENCE_GATING=false
CAIRN_LLM_EVENT_DIGEST=true

# CORS: comma-separated origins, or "*" to allow all (default)
# Example: https://cairn.example.com,http://localhost:3000
CAIRN_CORS_ORIGINS=*

# API Authentication (optional, off by default)
# When enabled, all /api routes require a valid key in the configured header.
# Health (/api/status) and docs (/api/swagger) are always exempt.
# MCP endpoint (/mcp) is not affected.
CAIRN_AUTH_ENABLED=false
CAIRN_API_KEY=
CAIRN_AUTH_HEADER=X-API-Key

# Event archive: file-based JSONL archive of session events
CAIRN_EVENT_ARCHIVE_DIR=/data/events

# Embedding
# Backend: "local" (SentenceTransformer, default). Custom providers can be
# registered via the Python API.
CAIRN_EMBEDDING_BACKEND=local
CAIRN_EMBEDDING_MODEL=all-MiniLM-L6-v2
CAIRN_EMBEDDING_DIMENSIONS=384

# Ingestion chunking (for large document ingestion)
CAIRN_INGEST_CHUNK_SIZE=512
CAIRN_INGEST_CHUNK_OVERLAP=64
